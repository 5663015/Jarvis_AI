# 介绍

本项目旨在总结机器学习的一些基础知识，偏重于面试中会问到的问题。一共分成了机器学习基础、数学相关、贝叶斯方法、线性回归、逻辑回归、SVM、集成学习、树模型、降维、聚类、强化学习、深度学习、优化算法等部分，每部分单独为一个pdf文件，不定期更新。

- 2019年2月5日：添加了深度学习的预览版本（收录了部分问题，未添加参考文献）


具体问题目录及更新情况如下所示：

- **机器学习基础**
  - [x] 判别模型与生成模型的区别
  - [x] 线性分类和非线性分类各有哪些模型
  - [x] 机器学习中常用的损失函数有哪些
  - [ ] 特征选择的方法
  - [x] 介绍一个完整的机器学习项目流程
  - [ ] 总结各类机器学习算法的特点
  - [x] 常用的模型性能度量方法
  - [x] 误差、方差和偏差的区别、联系
  - [x] 解释k折交叉验证
  - [x] 什么是经验误差、泛化误差
  - [x] 如何解决过拟合、欠拟合
  - [ ] 类别不平衡问题解决方法
  - [x] 在时序数据集上如何使用交叉验证技术
  
- **数学相关**
  - [x] 泰勒展开在机器学习中的应用
  - [x] 协方差和相关系数的区别
  - [x] 解释范数
  - [ ] 特征分解的方法
  - [ ] 最大似然估计和最大后验概率的区别
  - [x] 概率和似然的区别
  - [x] kl散度和js散度
 
- **贝叶斯方法**
  - [x] 解释贝叶斯公式和朴素贝叶斯分类
  - [x] 朴素贝叶斯分类器出现估计概率值为0如何处理
  - [ ] 朴素贝叶斯分类器的优化和特殊情况处理

- **线性回归**
  - [x] 线性回归的原理
  - [x] 线性回归为何要求变量服从正态分布
  
- **逻辑回归**
  - [x] LR推导
  - [ ] LR如何处理多分类
  - [x] LR的优缺点
  - [x] 朴素贝叶斯和LR的区别
  - [ ] LR是否可以核函数
  - [ ] 为什么LR归一化或取对数，为什么LR把特征离散化后效果更好
  - [ ] 为什么LR可以做CTR预估
  - [ ] 线性回归于逻辑回归的区别
  - [x] L1正则化产生稀疏的原因
  - [ ] LASSO回归
  - [ ] 岭回归

- **SVM**
  - [ ] svm推导
  - [ ] svm引入拉格朗日优化方法的原因
  - [ ] svm里的参数C的作用
  - [ ] 解释原问题和对偶问题
  - [ ] 解释KKT限制条件
  - [ ] 核函数的作用及常用的核函数
  - [ ] svm如何解决多分类问题
  - [ ] svm如何解决回归问题
  - [ ] svm的优缺点
  - [ ] 加大训练数据量一定会提高svm的准确率吗
  - [ ] LR和svm的区别、联系
  - [ ] svm和感知机、神经网络的联系
  
- **集成学习**
  - [ ] 解释bagging、boosting、stacking，以及它们的对比
  - [x] 从方差、偏差角度解释bagging和boosting
  - [ ] 集成学习是否一定会有提升
  - [ ] 随机森林原理
  - [ ] 随机森林分类效果的影响因素
  - [ ] 随机森林的优缺点
  
- **树模型**
  - [ ] 决策树算法的有趣诶单
  - [x] 熵的概念及理解
  - [x] 信息增益的概念及理解
  - [x] 基尼指数的概念及理解
  - [ ] ID3、C4.5、CART三种树模型的原理及对比
  - [ ] 决策树如何处理连续值
  - [ ] 决策树如何处理缺失值
  - [ ] CART分类树和回归树的区别
  - [ ] 决策树如何剪枝
  - [ ] GBDT算法原理
  - [ ] GBDT与随机森林的区别与联系
  - [ ] 为何GBDT用负梯度近似残差
  - [ ] GBDT的优缺点
  - [ ] XGBoost的原理
  - [ ] XGBoost如何调参
  - [ ] XGBoost与GBDT的区别
  - [ ] XGBoost的内部优化
  - [ ] lightgbm原理
  - [ ] xgboost与lightgbm的区别
  - [ ] 决策树如何避免过拟合
  
- **降维**
  - [ ] PCA原理
  - [ ] 线性判别分析LDA原理


- **EM算法**
  - [ ] EM原理
  - [ ] 采样EM算法求解的模型有哪些，为什么不用牛顿法或梯度下降法
  - [ ] 高斯混合模型
  
- **聚类**
  - [ ] kmeans原理、优缺点
  - [ ] kmeans中的k怎么设置
  - [ ] 怎么评价kmeans的聚类效果
  - [ ] kmeans算法的改进
  - [ ] EM与kmeans的关系
  - [ ] 密度聚类算法原理
  - [ ] DBSCAN算法原理
  
- **强化学习**
  - [x] 马尔科夫决策过程
  - [ ] 策略迭代
  - [ ] 值迭代
  - [ ] 策略梯度
  
- **深度学习**
  - [ ] 激活函数及其选择
  - [ ] BP算法原理
  - [x] 卷积公式
  - [x] 卷积的特点、作用
  - [x] 1x1卷积的作用
  - [ ] 如何减少卷积的参数
  - [ ] 推导CNN的反向传播过程
  - [x] RNN原理
  - [x] LSTM原理
  - [ ] 推导LSTM反向传播过程
  - [ ] 为什么会出现梯度爆炸或消失，如何解决
  - [ ] 导致模型不收敛的原因
  - [ ] 深度学习中的正则化
  - [ ] 如何根据损失变化调整网络
  - [x] 参数初始化策略
  - [x] BN原理及其作用
  - [ ] 如何理解Group Normalization
  - [ ] VGG为何使用2个3x3的卷积核
  - [ ] Relu想比喻sigmoid、tanh的优缺点
  - [x] dropout的原理
  - [x] 解释空洞卷积
  - [x] 如何理解转置卷积
  - [x] 什么是分组卷积
  - [x] 什么是深度可分离卷积
  - [ ] embedding层的作用
  - [ ] attention机制
  
- **优化**
  - [x] 如何判断函数凸或非凸
  - [ ] 梯度下降的优缺点
  - [x] 梯度下降算法的原理及各变种
  - [ ] 如果有若干个极小值点，如何避免陷入局部最优
  - [ ] 最速下降法和共轭梯度法
  - [x] 理解全局最优与局部最优
  
# 联系我们

本项目由“[贾维斯的小屋](https://5663015.github.io/)”AI社区创建，欢迎提出批评指正意见，也欢迎一起来维护此项目~

欢迎添加微信：wuan3076，加入“贾维斯的小屋-AI极客”微信讨论群~

# 参考文献



【注】本项目的问题答案很多参考了网上的资料，如果有侵权或未标明出处请联系我们

